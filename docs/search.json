[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "1 Introduction\n\nThis book is a learning diary for CASA0023 Remotely Sensing Cities and Environments\n\n\n\n\nWhat is Remote Sensing"
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "2  Week 1: Getting started with remote sensing",
    "section": "2.1 Summary",
    "text": "2.1 Summary\n\n2.1.1 The concept of remote sensing\nIn contrast to in situ or on-site observation, remote sensing is the process of gathering data about a phenomenon or object without actually coming into touch with it. The phrase is used particularly in reference to learning more about Earth and other planets.\nThe phrase “remote sensing” today typically refers to the detection and classification of Earthly objects using satellite- or aircraft-based sensor technologies. Based on transmitted signals, it encompasses the atmosphere, the surface, and the oceans (e.g. electromagnetic radiation). It can be divided into “active” and “passive” remote sensing (where a signal is emitted to the object by a satellite or aircraft and its reflection is detected by the sensor) (when the reflection of sunlight is detected by the sensor)\n\n\n\n2.1.2 Piece of work:\nIn the first chapter, I learned how to access remote sensing data using two open source databases, Landsat and Copernicus Open Access Hub, their existence make it easy for me to access remote sensing data from anywhere in the world.\nSecondly, I learned how to use QGIS, SNAP and some packages in R for simple raster processing calculations. I used my own case and followed the instructions step by step, learning a lot about the terminology, gaining some knowledge about remote sensing and statistics and also having some questions."
  },
  {
    "objectID": "week1.html#application",
    "href": "week1.html#application",
    "title": "2  Week 1: Getting started with remote sensing",
    "section": "2.2 Application",
    "text": "2.2 Application\n\n2.2.1 QGIS\nWhen learning QGIS, I used as an example a map of Zhenjiang and Changzhou, two cities in Jiangsu Province, China, bordered by the Yangtze River to the north, from Copernicus Open Access Hub. Then, merged the B2 (blue), B3 (green), B4 (red) and B8 (NIR) in R10m with multiband colour, and knew the details in different enhancement options. Then, I learned about difference between R10m and R20m. Generally, upsampling is used to complement the situation when the pixel resolution is greater than the source resolution, and a common interpolation method is nearest neighbour, while downsampling is used when pixel resolution is smaller than source resolution. Here is the mechanism of nearest neighbour:\n\\(X_{src}=X_{dst}*\\frac{Width_{src}}{Width_{dst}}\\)\n\\(Y_{src}=Y_{dst}*\\frac{Height_{src}}{Height_{dst}}\\)\nIf the calculation results in a fractional number, then either remove the decimal part or round up to get the same value as the original dotted pixel. This method is very simple but inaccurate, the zoomed in image has a very bad mosaic and the zoomed out image has a very bad distortion. The problem lies in the treatment of the fractional part, which is equivalent to the process of colour gradation and cannot be fully equated with the existing colour of pixel. It is possible to use bilinear interpolation, considering the four values around the point that needs to be scaled, to see which value has more influence on it, and to give it more weight, I saw the option of bilinear interpolation then tried it. In addition, cubic has more accurate result because non-linear function can fit better but more complex.\n\n\n2.2.2 SNAP\nThe sRBG standard colour space can be used for display, network transmission and to convert colour levels on other devices into a colour space that is recognisable to the computer by means of the sRGB conversion function. This is why the brightness levels in the range [0,4095] on the Sentinel-2 can be converted to values in the range [0,255].\nWhen using SNAP, 432 nm means this wavelength is 432 nm, it is a coastal aerosol wave which wavelength is lessmthan blue wave. I like the colour composition function, which allows me to highlight different sections depending on my needs.\n\n\n\nB8-B4-B3\n\n\nHere is the scatter plot of my example:\n\n\n\nscatterplot\n\n\nIt can be seen that map of Zhenjiang and Changzhou has high brightness level, which means it is a city area, and it also shows a lot of wet bare soil, we can assume that these wet soils may be more suitable for cultivation, the overall biomass is not very large and, most notably, there is also a lot of dry bare soil lack of vegetation cover, presumably non-agricultural land.\nWhen using mask function, I got the administrative divisions of these municipalities because I downloaded the administrative divisions of China from GADM data:\n\n\n\nadministrative divisions\n\n\n(yellow: Zhenjiang, pink:Changzhou, red: Taizhou, green:Wuxi, blue: Yangzhou)\nAfter resampling and downscaling the data I got this tasselled plot, compared to the previous one I found that the downscaling had reduced the urban part, so the plot was significantly less bright, and there was more wet bare soil present than the previous one.\n\n\n\nscatterplot\n\n\nGenerally speaking, I prefer using SNAP than QGIS because it has more vivid chart and we can evaluate the local soil conditions, the size of the city and the biomass.\n\n\n2.2.3 R script\nWhen learning the code to deal with data from Landsat and Sentinel, I found a final paired t-test is performed to test whether the actual means of the urban land type from Landsat and Sentinel are equal, the degree of freedom from the results is 5970, p-value is much smaller than 0.05, which means the actual means of two groups are quite different, and at the same time, this calculation also shows the estimate mean of two samples. There are 95% probability that the true mean is between -9300.307 and -9198.368. data from Sentinel and Landsat is different."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "2  Week 1: Getting started with remote sensing",
    "section": "2.3 Reflection",
    "text": "2.3 Reflection\nRemote sensing technology is now widely used in scientific research and practice, for land planning, geological exploration and forest fire prevention, so learning to make remote sensing maps is an essential skill.\nThere are several areas of interest to me, one of them is how PCA can be used to reduce the dimensionality of data, particularly in image processing, and I will be doing further study and figuring this out. Then, I would like to know if the image data from satellite remote sensing can be used in AI, as there are already mature deep learning algorithms with a high accuracy rate for image recognition, how to use these images to achieve functions such as automatic recognition and automatic monitoring."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "3  Week 2: Portfolio",
    "section": "",
    "text": "Here is the link of presentation which is created via Xaringan:\nPresentation"
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "4  Week 3: Corrections",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nThis week I learned something about the corrections, the corrections can be divided into four aspects, geometric correction, atmospheric correction, orthorectification correction and radiometric calibration.\n\n4.1.1 Geometric correction\nWhy we need to do the geometric corrections:\n\nThe satellite has view angles when watching the Earth\nThe undulating nature of the terrain (topography)\nWind\nRotation of Earth\n\nThe methods to do the geometric correction:\n\nUse linear regression model to find the relationship between original points and distorted points\nBased on the linear relationship, it is better to use polynomial algorithms because it allows for a certain amount of curvature, it will be more accurate\nUse Thin Plate Spline algorithms to introduce the local deformations\n\n\n\n4.1.2 Atmospheric correction\nWe need to do the atmospheric correction to reduce the effect from path radiance and the haze created by absorption and scattering to get more accurate image\nThe methods to do the atmospheric correction:\n\nDark object subtraction (DOS), search the darkest value of image and subtract that from each pixel\nPsuedo-invariant features (PIFs) use the linear function to adjust the image\nACORN, FLAASH, QUAC, ATCOR\n\n\n\n4.1.3 Orthorectification correction\nThe aim of orthorectification correction:\n\nProvide the coordinates to an image\nRemove the distortions, make pixel viewed at straight down\n\nThe methods to do the orthorectification correction:\n\nConsider the solar zenith angle and solar azimuth angle to do the cosine correction\nThe software we can use QGIS, R packages like RStoolbox\n\n\n\n4.1.4 Radiometric calibration\nThe aim of radiometric calibration:\n\nConvert the digital number sensors captured from the image brightness (without units) to spectral radiance (with units)\n\nMethods to do the radiometric calbration:\n\nUse converting function"
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "4  Week 3: Corrections",
    "section": "4.2 Application",
    "text": "4.2 Application\n\n4.2.1 DOS and radiance\nI used the remote sensing image (LC08_L1TP_119038_20230104_20230111_02_T1.txt downloaded from USGS EarthExplorer) to calculate the pixel value of surface reflections generated from scattering by DOS:\ninstall.packages(\"rgdal\")\ninstall.packages(\"RStoolbox\")\n\nlibrary(terra)\nlibrary(raster)\nlibrary(RStoolbox)\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(rgdal)\n\n## Import meta-data and bands based on MTL file\nmtlFile  <- (\"D:\\\\Remote Sensing\\\\week3\\\\LC08_L1TP_119038_20230104_20230111_02_T1\\\\LC08_L1TP_119038_20230104_20230111_02_T1_MTL.txt\")\n\nmetaData <- readMeta(mtlFile)\nlsatMeta  <- stackMeta(metaData)\n\n# surface reflection with DOS\nl8_boa_ref <- radCor(lsatMeta, metaData, method = \"dos\")\n\n# write to local dos\nterra::writeRaster(l8_boa_ref, datatype=\"FLT4S\", filename = \"D:\\\\Remote Sensing\\\\week3\\\\LC08_L1TP_119038_20230104_20230111_02_T1\\\\l8_boa_ref.tif\", format = \"GTiff\", overwrite=TRUE)\n\n# Radiance \nlsat_rad <- radCor(lsatMeta, metaData = metaData, method = \"rad\")\n\nterra::writeRaster(lsat_rad, datatype=\"FLT4S\", filename = \"D:\\\\Remote Sensing\\\\week3\\\\LC08_L1TP_119038_20230104_20230111_02_T1\\\\lsat_rad.tif\", format = \"GTiff\", overwrite=TRUE)\n\n\n\nl8_boa_ref\n\n\nConclusion:\nThere are several methods to execute the atmosphereic corrections (from digital number to reflectance), two typical examples are DOS and converting radiance to reflectance. In this case, the difference between radiance to reflectance and DOS correction is not significant and both can be corrected accurately. Actually, When the atmosphere is thinner or when the weather is good, the influence of the atmosphere on the ground radiation is less, the TOA and BOA are close to each other, a small part of the light reaches the sensor through the path radiance, so the less dark object needs to be substracted, the effect is close to the radiance. On the contrary, the more pixels need to be corrected the more obvious the effect of the corrected image is compared to the pre-correction image.\n# hazeDN\n\nhazeDN <- RStoolbox::estimateHaze(lsatMeta, hazeBands = 2:4, darkProp = 0.01, plot = TRUE)\n\nlsat_sref <- radCor(lsatMeta, metaData = metaData, method = \"dos\", \n                    hazeValues = hazeDN, hazeBands = 2:4)\n\nterra::writeRaster(lsat_sref, datatype=\"FLT4S\", filename = \"D:\\\\Remote Sensing\\\\week3\\\\LC08_L1TP_119038_20230104_20230111_02_T1\\\\lsat_sref.tif\", format = \"GTiff\", overwrite=TRUE)\n\n\n\nresult of hazeDN (3 Bands)\n\n\n\n\n4.2.2 Merging imagery\nLandsat8, Landsat9 and data preparation\n# List your raster files excluding band 8 using the patter argument\nlistlandsat_8<-dir_info(\"D:\\\\Remote Sensing\\\\week3\\\\LC08_L2SP_119038_20230104_20230111_02_T1\")%>%\n  dplyr::filter(str_detect(path, \"[B123456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  terra::rast()\n\n# List your raster files excluding band 8 using the patter argument\nlistlandsat_9<-dir_info(\n\"D:\\\\Remote Sensing\\\\week3\\\\LC08_L2SP_119038_20230104_20230111_02_T1\"\n)%>%\n  dplyr::filter(str_detect(path, \"[1B23456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  terra::rast()\n\n# data preparation\nm1 <- terra::mosaic(listlandsat_8, listlandsat_9, fun=\"mean\")\n\n\n4.2.3 Enhancement\nCalculate the NDVI (Normalized Difference Vegetation Index)\nm1_NDVI <- (m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5 - m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4) / (m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5 + m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4)\n\nm1_NDVI %>%\n  plot(.)\n\n\n\nresult of m1_NDVI\n\n\nSet m1_NDVI is not greater than 0.2\nveg <- m1_NDVI %>%\n  terra::classify(., cbind(-Inf, 0.2, NA))\n\nveg %>%\n  plot(.)\nThe highlight area of healthy vegetation:  From this map, the vegetation in the Taihu Lake basin is more concentrated to the west of the lake, where the most vegetation is found and where there is less population, while the area north of the lake to the south of the Yangtze River is less vegetated due to the high level of urbanisation, while the area north of the Yangtze River is relatively more vegetated.\nCalculate the NDMI (Normalized Difference Moisture Index) Use band 5 and band 6 on the occasion of Landsat8:\n\\(NDMI = (Band5 - Band6) / (Band5 + Band6)\\)\nm1_NDMI <- (m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5 - m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B6) / (m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5 + m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B6)\n\nm1_NDMI %>%\n  plot(.)\n\n\n\nresult of m1_NDMI\n\n\nmoi <- m1_NDMI %>%\n  terra::classify(., cbind(-Inf, 0.05, NA))\n\nmoi %>%\n  plot(.)\n ### Filtering\n# for a 3 by 3 filter on Band4\nm1_filter <- terra::focal(m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4, w=matrix(nrow=3,ncol=3))\n\n\n4.2.4 Texture analysis\nGenerate the glcm.red and glcm.nir from Band4 and Band5, see the result under different statistical indicators\ninstall.packages(\"glcm\")\n\n\nlibrary(glcm)\nlibrary(raster)\n\n# band4 red, band5 NIR\n\nband4_raster <- raster::raster(m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4)\nband5_raster <- raster::raster(m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5)\n\nglcm.red <- glcm(band4_raster,\n                   window = c(7, 7),\n                   #shift=list(c(0,1), c(1,1), c(1,0), c(1,-1)), \n                   statistics = c(\"mean\",\n                                  \"variance\",\n                                  \"homogeneity\",\n                                  \"contrast\",\n                                  \"entropy\", \n                                  \"dissimilarity\",\n                                  \"second_moment\", \n                                  \"correlation\"))\n\n\nglcm.nir <- glcm(band5_raster,\n                   window = c(7, 7),\n                   #shift=list(c(0,1), c(1,1), c(1,0), c(1,-1)), \n                   statistics = c(\"mean\",\n                                  \"variance\",\n                                  \"homogeneity\",\n                                  \"contrast\",\n                                  \"entropy\", \n                                  \"dissimilarity\",\n                                  \"second_moment\", \n                                  \"correlation\"))\n\n\n\nplot(glcm.red)\n\n\n\nresult of glcm.red\n\n\nplot(glcm.nir)\n\n\n\nresult of glcm_nir\n\n\n\n\n4.2.5 Data fusion and PCA\n# for the next step of PCA we need to keep this in a raster format\n\n# m1_raster, glcm.red\nm1_raster <- stack(m1)\nFuse <- stack(m1_raster, glcm.red)\n\nFuse_3_bands <- stack(Fuse$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4, Fuse$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5, Fuse$glcm_homogeneity)\n\nscale_fuse <- scale(Fuse_3_bands)\n\n# m1_raster, glcm.red\npca <- rasterPCA(Fuse, nSamples =100, spca = TRUE)\n\nsummary(pca$model)\n\n\n\nresult of pca (m1_raster, glcm.red)\n\n\nIt can be seen that in my study area, the component 1 accounts for 54.20%, and the summary of component 1 to component 3 accounts for over 90%, component 1 can explain the 54.2% data from entire dataset\nplot(pca$map)\n\n\n\nplot of PCA each component (m1_raster, glcm.red)\n\n\n# glcm.red, glcm.nir\nFuse <- scale(stack(glcm.red, glcm.nir))\n\npca <- rasterPCA(Fuse, nSamples = 100, spca = TRUE)\n                 \nsummary(pca$model)\n\n\n\nresult of pca (glcm.red, glcm.nir)\n\n\nThe component 1 of glcm.red and glcm.nir is 36.63%, the result is not so good\nplot(pca$map)"
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "4  Week 3: Corrections",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nIn this week, I learned the knowledge about the corrections, and the methods about corrections, the distortion of the images from remote sensing is mainly due to the presence of sensor observations in terms of declination, topography, wind and so on. Their correction uses statistical knowledge and principles (such as PCA, linear regression) as well as knowledge of spatial geometry.\nI think it is useful for the further study and work, for example, correcting the satellite image to satisfy people’s demand."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "5  week 5: Policy",
    "section": "",
    "text": "6 city plan and application of remote sensing for London"
  },
  {
    "objectID": "week4.html#summary",
    "href": "week4.html#summary",
    "title": "5  week 4: Policy",
    "section": "5.1 Summary",
    "text": "5.1 Summary\n\n5.1.1 London City Plan – zero-carbon policy\nWith the increasing scale of greenhouse gas emissions in recent years and the increasing severity of the global warming situation, London 2021 city plan sets a significant goal to minimise the greenhouse gas emission, the detailed aims are below:\n\ncity development should be net-zero carbon: reducing greenhouse gas emission in operation; minimise both annual and peak energy demand according to several energy hierarchies:\n\n\nbe lean: less energy used and energy demand management\nbe clean: fully use local energy (secondary energy explicit) and use energy more efficiently and cleanly.\nbe green: add the opportunity for using renewable energy, provide producing, storing energy on-site\nbe seen: monitor, verify and report on energy performance.\n\n\n\n\nEnergy hierarchy framework. Source: London city plan 2021\n\n\n\nCity development proposals should include energy strategic and demonstrate how energy hierarchy framework includes the zero-carbon target.\nA minimum 35% reduction in energy consumption on site for major development program, 10% for residential developments and 15% for non-residential developments through energy efficiency measures should be achieved. Where this is difficult to achieve, there should be compensation for cash compensation for shortfalls or provide an alternative that will meet the target.\nEach borough government should establish and manage a carbon offset fund, ensure the capital should be invested to the carbon reduction direction, and all the operations should be monitored and reported to specific authorisations.\nCarbon reduction proposal should be quantified as separated to different plants, equipment and other responsible parties.\nWhole life-cycle carbon emissions should be calculated to satisfy the proposal of city development through a nationally recognised Whole Life-Cycle Carbon Assessment."
  },
  {
    "objectID": "week4.html#application",
    "href": "week4.html#application",
    "title": "5  week4: Policy",
    "section": "5.2 Application",
    "text": "5.2 Application\n\n5.2.1 Case 1: Summer heat spots from Landsat8 Thermal satellite data\nLandsat8 satellite can provide a high-resolution thermal data and use as a basis for mapping the spatial distribution of Great London Area (GLA) surface temperatures and identifying those urban hotspots. This dataset is a count of surface temperatures from the summer months (June, July and August only) between 2016 and 2020, obtained from landsat8 thermal infrared imagery. In addition to the five-year average temperature for the area, the dataset also includes maximum and minimum temperatures and standard deviations to provide a visual representation of the magnitude and range of temperature changes over the five-year period in GLA.\nHigher temperatures tend to occur in densely populated areas.\n\n\n\nheat spot of London summer temperature. Data source: Major Summer Heat Spot London Datastore\n\n\nAs can be seen from the map, the population is predominantly distributed between the north and south banks of the Thames, with the population on the north bank being greater than that on the south bank. The green belt around London can be seen more clearly on the map. The population of the north bank is more dense in the east than in the west. The density of population is high from the City of London all the way north, and is also high in north-east London, decreasing further east. Conversely, the centre of the South Bank is more densely populated, while the east and west are less dense. If Greater London continues to expand, the north-east and south-east would be a better choice.\nTake the City of London for example, some areas saw the highest temperatures compared with the whole city, such as Euston Station east of Regent’s Park, and most of the high heat areas were in high-traffic areas such as train stations, tube stations, shopping malls and airports.\n\n\n\nheat spot in City of London\n\n\nIn addition to population density affecting surface temperatures, industry, airport also have an impact on surface temperatures, with the highest surface temperatures occurring in the five year period shown below at the Dagenham engine plant, ocado customer fullfilment centre, ExCel International Convention and Exhibition Centre along the Thames in east London, and Heathrow Airport to the west.\n\n\n\nHighest temperature areas (east)\n\n\n\n\n\nHighest temperature areas (west)\n\n\nKnowing the temperature changes will help to understand the population distribution in GLA, which in turn will allow transport to be organised to develop faster and more accessible transport in dense areas, to develop infrastructure and to reduce the pressure on existing transport (for example, the central line passes through areas of great population density, and the central line is the oldest tube in London and still the busiest in London). The central line, for example, is the oldest and still the busiest tube in London, so it has to take on a lot of traffic pressure). In addition, more polluting facilities such as factories can be relocated to less populated areas. Knowing the distribution of temperatures can also control the creation of high temperature situations, such as the rare high temperatures in the London area in the summer of 2022.\n\n\n5.2.2 case 2: Remote sensing of motor vehicle emissions in London\nIn response to a call for collaboration between the Mayor of London and the TRUE initiative, carbon emissions from passing vehicles were tested using remote sensing technology at nine sites across Greater London between 2017 and 2018, with carbon emissions data recorded for over 100,000 vehicles, The experiment focused on petrol and diesel vehicles in the London area, with vehicle types including passenger cars, buses, light passenger vehicles, trucks and motorbikes, and measured emissions of carbon oxides and nitrogen oxides.\nThe remote sensing equipment used for the experiments was the Opus AccuScan RSD5000, which was the first to test exhaust gases in three main ways:\n\nThe device emits infrared and ultraviolet light velocities that pass through vehicle emissions, measuring the attenuation of these beams, instant vehicle emissions, the device measures nitrogen oxides and carbon oxides, opacity is measured as a proxy for respirable particulate matter, the device emits a frequency of 200 Hz and can measure 100 times in 0.5 seconds.\nMeasurement of the instantaneous acceleration of the vehicle as a measure of the engine load, which is related to the instantaneous emission rate.\nOne camera is responsible for photographing the vehicle licence plate for database comparison to determine its model, displacement standard.\n\nThe general conditions of testing vehicles\n\n\n\nCharacteristic of testing vehicles. Soruce: Dallmann et al., 2018\n\n\nResult:\n\nDiesel passenger cars are six to seven times more likely to emit nitrogen oxides than petrol passenger cars.\n\n\n\n\n6 standard Euro NOx emission. Source: Dallmann et al., 2018\n\n\n\nEuro5 and Euro6 diesel engines emit significantly more nitrogen oxides than Euro3 and Euro4, while petrol engines emit the similar amount\n\n\n\n\naverage distance NOx emission for different vehicle family. Source: Dallmann et al., 2018\n\n\nTherefore, petrol vehicles outperform diesel vehicles and to achieve carbon emission reductions, the number of diesel vehicles needs to be limited and Euro3 or Euro4 diesel engines should be promoted instead of Euro5 and Euro6."
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "5  week 4: Policy",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nIn this week, I focused on the specific city development goal, minimising carbon emission, building zero-carbon city, to find cases for practice, and use the heat spot distribution map and identification of high-emitted vehicle to demonstrate how these goals are met within the energy hierarchy framework.\nWith the policy and the measurement mentioned on the London city plan 2021, the borough governments will take actions to control high-emitted areas and objects, for example, set the highest emission restriction for plants and equipment, and decrease the peak energy demand. restrict vehicles (time restriction, location restriction) to the city center area, encourage the replacement of Euro engine by providing benefits and making compensation."
  },
  {
    "objectID": "week4.html#application-case-study",
    "href": "week4.html#application-case-study",
    "title": "5  week4: Policy",
    "section": "5.2 Application (case study)",
    "text": "5.2 Application (case study)\nCase 1:"
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "6  week 5: Google Earth Engine",
    "section": "6.1 Summary",
    "text": "6.1 Summary\n\n6.1.1 Introduction about Google Earth Engine\nGoogle Earth Engine is a cloud-based platform for geospatial data analysis and visualization. It provides access to a vast repository of satellite imagery and geospatial data, as well as a powerful suite of tools for processing and analyzing this data.\nThe advantages of Earth Engine is scalability. The platform can handle extremely large datasets and perform complex analysis in near real-time, making it well suited for monitoring changes on a global scale. Earth Engine also offers an easy-to-use programming interface (use command-based programming, env: javascript), allowing users to write custom scripts and algorithms to perform specific analysis tasks.\nGoogle Earth Engine is also widely used by government agencies, NGOs, and academic institutions for a variety of purposes, including environmental monitoring, natural resource management, and disaster response. The platform is free to use for non-commercial purposes, and access to its data and tools can be granted through a simple application process.\n\n\n6.1.2 Application of Google Earth Engine\n\nEurope’s Air Quality\n\nFrom Google Earth Engine, we can see the air quality monitor of over than 30 countries in Europe, it has several standards, the maximum cloud allowance, reducer scale, we can compare the absolute difference and relative difference, and find the top and the last countries.\n\nNDVI slider\n\nThis is the map of vegetation distribution all over the world, From this map we can see where on which continents the vegetation is dense and where it is likely to be desert and saline, and it also supports the comparison between different years to see the trend of vegetation distribution on the surface of earth."
  },
  {
    "objectID": "week5.html#application",
    "href": "week5.html#application",
    "title": "6  week 5: Google Earth Engine",
    "section": "6.2 Application",
    "text": "6.2 Application\n\n6.2.1 Single point selection\nI used Changzhou city, Jiangsu Province, China as my ROI, and select the point [119.9698545084502,31.780198098728107]\n\n\n\nROI\n\n\nThen use Landsat9 surface reflectance dataset (LANDSAT/LC09/C02/T1_L2), filter date is from 2020-01-01 to 2023-02-10, but when cloud cover is 0.1, I can not get any image, so I tried to adjust the cloud cover parameter to 0.4, I got two images, the cloud cover is 0.24 and 0.32, that is not cloud free Landsat9 scene. Until now, I have got two problems:\n\nCloud cover is larger than 0.1\nOnly show one single point, not the whole city\n\n\n\n\ncloud cover\n\n\nTo fix these problem, I use GADM data and choose gadm4_CHN_2 to display the city boundary and load the image:\nvar Changzhou = ee.FeatureCollection('users/giscodingmo/gadm41_CHN_2')\n    .filter('NL_NAME_1 == \"常州市\"');\n\nvar oneimage_study_area_cloud = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n.filterDate('2021-01-01', '2022-12-10')\n.filterBounds(Changzhou)  // Intersecting ROI\n.filter(ee.Filter.lt(\"CLOUD_COVER\", 0.1));\n\n\n// load the image\nvar image_120_38 = ee.Image('LANDSAT/LC08/C02/T1_L2/LC08_120038_20211004')\n\n// add the image on the map\nMap.addLayer(one_image, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]}, \"Landsat 8\")\nRresult: only one image in Landsat8 can reach the standard, and the image id is LANDSAT/LC08/C02/T1_L2/LC08_120038_20211004, which means path 120, row 38\n\n\n\npath and row\n\n\n\n\n6.2.2 Discussion of median method\nInitially, I use imageCollection.reduce() function (median method) to reduce the images:\nvar median = oneimage_study_area_cloud.reduce(ee.Reducer.median());\n\n// print the image info\nprint(median, \"median\")\nMedian method is effective when the classification task is between those objects which have giant jump such as forest and non-forest area, water and non-water area.\nIf the gap is not obvious, it is difficult to use median method to integrate images. To improve the accuracy of the classification, an improved method is to add other quartiles to the median, e.g. 25% quartile, 75% quartile. Another method is using seasonal median (spring, summer, fall, winter) to substitute single median, use a curve to fit the trend.\n// example of season medians\nfunction seasonComposite(start) {\n  var end = ee.Number(start).add(2)  \n  // transfer the data type\n  return collection\n  .filter(ee.Filter.calendarRange(start, end, \"month\"))\n  .median()\n}\n\n// call\nvar seasons = ee.List([1,4,7,10]).map(seasonComposite);\nvar composite = ee.ImageCollection(seasons).toBands();\n\n\n6.2.3 Better images\n\n6.2.3.1 True color image\nwrite a function for the surface reflectance rate and temperature adjustment in Landsat Collection 2\n// Applies scaling factors in Collection 2\nfunction applyScaleFactors(image) {\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  var thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n  return image.addBands(opticalBands, null, true)\n              .addBands(thermalBands, null, true);\n}\n\n// call our collection to the function and assign it to a new variable \nvar oneimage_study_area_cloud_scale = oneimage_study_area_cloud.map(applyScaleFactors);\n\n// apply the median reducer\nvar oneimage_study_area_cloud_scale_median = oneimage_study_area_cloud_scale.reduce(ee.Reducer.median());\n\nprint(oneimage_study_area_cloud_scale_median)\n\n\n// set up some of the visualisation paramters \nvar vis_params = {\n  bands: ['SR_B4_median', 'SR_B3_median', 'SR_B2_median'],\n  min: 0.0,\n  max: 0.3,\n};\n\n// add a layer to the map\nMap.addLayer(oneimage_study_area_cloud_scale_median, vis_params, 'True Color (432)');\nResult:\n\n\n\nvisualization by true color (432)\n\n\n\n\n6.2.3.2 Mosaic images\nvar mosaic = oneimage_study_area_cloud_scale.mosaic();\n\nvar vis_params2 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.addLayer(mosaic, vis_params2, 'spatial mosaic');\nResult:\n\n\n\nmosaic picture\n\n\nthere is no significant difference when using single image mosaic model and true color (432) model.\n\n\n6.2.3.3 Clip image\nvar clip = meanImage.clip(Changzhou)\n  .select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']);\n\nvar vis_params3 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0,\n  max: 0.3,\n};\n\n// map the layer\nMap.addLayer(clip, vis_params3, 'clip');\nResult: the image boundary is followed by city boundary\n\n\n\ncity boundary clip image\n\n\n\n\n\n6.2.4 Texture measures\n// based on the clip generated before\nvar glcm = clip.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'])\n  .multiply(1000)\n  .toUint16()\n  .glcmTexture({size: 1})\n  .select('SR_.._contrast|SR_.._diss')\n  .addBands(clip);\n  \n// add to the map, but change the range values  \nMap.addLayer(glcm, {min:15, max:650 }, 'glcm');\nResult:\n\n\n\ntexture adjustment\n\n\n\n\n\npixel distribution under glcm image\n\n\n\n\n6.2.5 PCA\nPrinciple component analysis code is here:\nvar scale = 30;\nvar bandNames = glcm.bandNames();\n\n// print(bandNames)\n\nvar region = Changzhou.geometry();\n// Map.centerObject(region, 10);\n// Map.addLayer(ee.Image().paint(region, 0, 2), {}, 'Region');\n\n// print(region, \"Changzhou_geometry\")\n// print(bandNames)\n\n// mean center the data and SD strech the princapal components \n// and an SD stretch of the principal components.\nvar meanDict = glcm.reduceRegion({\n    reducer: ee.Reducer.mean(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n});\n\nvar means = ee.Image.constant(meanDict.values(bandNames));\nvar centered = glcm.subtract(means);\n\n\n// This helper function returns a list of new band names.\nvar getNewBandNames = function(prefix) {\n  var seq = ee.List.sequence(1, bandNames.length());\n  return seq.map(function(b) {\n    return ee.String(prefix).cat(ee.Number(b).int());\n  });\n};\n\n\n// This function accepts mean centered imagery, a scale and\n// a region in which to perform the analysis.  It returns the\n// Principal Components (PC) in the region as a new image.\nvar getPrincipalComponents = function(centered, scale, region) {\n  // Collapse the bands of the image into a 1D array per pixel.\n  var arrays = centered.toArray();\n\n  // Compute the covariance of the bands within the region.\n  var covar = arrays.reduceRegion({\n    reducer: ee.Reducer.centeredCovariance(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n  });\n\n  // Get the 'array' covariance result and cast to an array.\n  // This represents the band-to-band covariance within the region.\n  var covarArray = ee.Array(covar.get('array'));\n\n  // Perform an eigen analysis and slice apart the values and vectors.\n  var eigens = covarArray.eigen();\n\n  // This is a P-length vector of Eigenvalues.\n  var eigenValues = eigens.slice(1, 0, 1);\n  // This is a PxP matrix with eigenvectors in rows.\n  \n  var eigenValuesList = eigenValues.toList().flatten()\n  var total = eigenValuesList.reduce(ee.Reducer.sum())\n  var percentageVariance = eigenValuesList.map(function(item) {\n  return (ee.Number(item).divide(total)).multiply(100).format('%.2f')\n    })\n  \n  print(\"percentageVariance\", percentageVariance)  \n\n  var eigenVectors = eigens.slice(1, 1);\n\n  // Convert the array image to 2D arrays for matrix computations.\n  var arrayImage = arrays.toArray(1);\n\n  // Left multiply the image array by the matrix of eigenvectors.\n  var principalComponents = ee.Image(eigenVectors).matrixMultiply(arrayImage);\n\n  // Turn the square roots of the Eigenvalues into a P-band image.\n  var sdImage = ee.Image(eigenValues.sqrt())\n    .arrayProject([0]).arrayFlatten([getNewBandNames('sd')]);\n\n  // Turn the PCs into a P-band image, normalized by SD.\n  return principalComponents\n    // Throw out an an unneeded dimension, [[]] -> [].\n    .arrayProject([0])\n    // Make the one band array image a multi-band image, [] -> image.\n    .arrayFlatten([getNewBandNames('pc')])\n    // Normalize the PCs by their SDs.\n    .divide(sdImage);\n};\n\n// Get the PCs at the specified scale and in the specified region\nvar pcImage = getPrincipalComponents(centered, scale, region);\n\n// Plot each PC as a new layer\nfor (var i = 0; i < bandNames.length().getInfo(); i++) {\n  var band = pcImage.bandNames().get(i).getInfo();\n  Map.addLayer(pcImage.select([band]), {min: -2, max: 2}, band);\n}\nResult: I got the pictures, the number is equal to the length of bandNames, in my case, the bandNames contains 21 elements.\n\n\n\nexample of PCA analysis\n\n\nThe first component can explain 63.68% of variance within the collection and the second component explains 26.66%, the third explains only 6.11%, therefore, I can just add pc1 and pc2 instead of the entire image, somethimes, pc3 can also be added to improve the accuracy, reaching 96.45%.\n\n\n\nPercentage variables of PCA\n\n\nMap.addLayer(pcImage, {bands: ['pca3', 'pc2', 'pc1'], min: -2, max: 2}, 'PCA bands 1, 2 and 3');\n\nMap.addLayer(pcImage, {bands: ['pc2', 'pc1'], min: -2, max: 2}, 'PCA bands 1, 2');\nComparison between 2 components and 3 components:\n\n\n\n2 components\n\n\n\n\n\n3 components\n\n\nExport image to drive\nvar PCA_out = pcImage.select(['pc1', 'pc2', 'pc3'])\n\nvar projection = PCA_out.select('pc1').projection().getInfo();\n\nvar bounds = Changzhou.geometry();\n\n// Export the image, specifying the CRS, transform, and region.\nExport.image.toDrive({\n  image: PCA_out,\n  description: 'PCA_Changzhou',\n  scale:30,\n  crs: projection.crs,\n  maxPixels: 100E10,\n  region: bounds\n});\n\n\n6.2.6 Band math\nCalculate the NDVI quickly, use blue to identify the water area, green to identify the forest or vegetation area and white to identify the town, city\n//NDVI\nvar NDVI_1 = clip.select('SR_B5').subtract(clip.select('SR_B4'))\n  .divide(clip.select('SR_5').add(clip.select('SR_B4')));\n\nMap.addLayer(NDVI_1, { min: -1, max: 1, palette: ['blue', 'white', 'green']}, 'NDVI');\n\n\n\nNDVI"
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "6  week 5: Google Earth Engine",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nIn this week, I learned knowledge in several aspects: * how to use Google Earth Engine to create points and analyse the ROI assisted with GADM boundary map.\n\nunderstood the advantages and disadvantages of using median methods to get the reduced images (advantages: fast calculation, be suitable for those objects which has giant difference; disadvantages: not accurate, can not reflect the trend of change) and knew the alternative and more accurate methods (multiple quantile methods: using 10% quantile, 25% quantile, 75% quantile, seasonal medians methods: using four medians to represent 4 seasons)\ncreate true color image, mosaic image, clip image and do the texture adjustment by glcm\nprinciple component analysis and find the explainable percentage, calculated NDVI and export them to local"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "CASA0023 Learning Diary",
    "section": "0.1 Introduction",
    "text": "0.1 Introduction\nThis book is a learning diary for CASA0023 Remotely Sensing Cities and Environments\n\n\n\nWhat is Remote Sensing"
  },
  {
    "objectID": "week4.html#application-on-carbon-emission",
    "href": "week4.html#application-on-carbon-emission",
    "title": "5  week 4: Policy",
    "section": "5.2 Application on carbon emission",
    "text": "5.2 Application on carbon emission\n\n5.2.1 Step 1: Heat spots identification\nLandsat8 satellite can provide a high-resolution thermal data and use as a basis for mapping the spatial distribution of Great London Area (GLA) surface temperatures and identifying those urban hotspots.\n\nDataset description:\n\nThis dataset is a count of surface temperatures from the summer months (June, July and August only) between 2016 and 2020, obtained from landsat8 thermal infrared imagery. In addition to the five-year average temperature for the area, the dataset also includes maximum and minimum temperatures and standard deviations to provide a visual representation of the magnitude and range of temperature changes over the five-year period in GLA.\n\nDistribution map and analysis\n\nHigher temperatures indicate places with high population density or dense plant and equipment, where greenhouse gas emissions are generally higher than in places with less.\n\n\n\nheat spot of London summer temperature. Data source: Major Summer Heat Spot London Datastore\n\n\nAs can be seen from the map, the population is predominantly distributed between the north and south banks of the Thames, with the population on the north bank being greater than that on the south bank. The green belt around London can be seen more clearly on the map. The population of the north bank is more dense in the east than in the west. The density of population is high from the City of London all the way north, and is also high in north-east London, decreasing further east. Conversely, the centre of the South Bank is more densely populated, while the east and west are less dense. If Greater London continues to expand, the north-east and south-east would be a better choice.\nTake the City of London for example, some areas saw the highest temperatures compared with the whole city, such as Euston Station east of Regent’s Park, and most of the high heat areas were in high-traffic areas such as train stations, tube stations, shopping malls and airports.\n\n\n\nheat spot in City of London\n\n\nIn addition to population density affecting surface temperatures, industry, airport also have an impact on surface temperatures, with the highest surface temperatures occurring in the five year period shown below at the Dagenham engine plant, ocado customer fullfilment centre, ExCel International Convention and Exhibition Centre along the Thames in east London, and Heathrow Airport to the west.\n\n\n\nHighest temperature areas (east)\n\n\n\n\n\nHighest temperature areas (west)\n\n\nKnowing the temperature changes will help to understand the population distribution in GLA, which in turn will allow transport to be organised to develop faster and more accessible transport in dense areas, to develop infrastructure and to reduce the pressure on existing transport (for example, the central line passes through areas of great population density, and the central line is the oldest tube in London and still the busiest in London). The central line, for example, is the oldest and still the busiest tube in London, so it has to take on a lot of traffic pressure). In addition, more polluting facilities such as factories can be relocated to less populated areas. Knowing the distribution of temperatures can also control the creation of high temperature situations, such as the rare high temperatures in the London area in the summer of 2022.\n\n\n5.2.2 Step 2: Identification of high-emitted vehicles\nIn response to a call for collaboration between the Mayor of London and the TRUE initiative, carbon emissions from passing vehicles were tested using remote sensing technology at nine sites across Greater London between 2017 and 2018, with carbon emissions data recorded for over 100,000 vehicles, The experiment focused on petrol and diesel vehicles in the London area, with vehicle types including passenger cars, buses, light passenger vehicles, trucks and motorbikes, and measured emissions of carbon oxides and nitrogen oxides.\n\nDataset example\n\n\n\n\ndata descriptive statistics. Soruce: Dallmann et al., 2018\n\n\n\nRemote sensing equipment introduction\n\nThe remote sensing equipment used for the experiments was the Opus AccuScan RSD5000, which was the first to test exhaust gases in three main ways:\n\nThe device emits infrared and ultraviolet light velocities that pass through vehicle emissions, measuring the attenuation of these beams, instant vehicle emissions, the device measures nitrogen oxides and carbon oxides, opacity is measured as a proxy for respirable particulate matter, the device emits a frequency of 200 Hz and can measure 100 times in 0.5 seconds.\nMeasurement of the instantaneous acceleration of the vehicle as a measure of the engine load, which is related to the instantaneous emission rate.\nOne camera is responsible for photographing the vehicle licence plate for database comparison to determine its model, displacement standard.\n\n\n\n\nremote sensing equipment\n\n\n\nThe general conditions of testing vehicles\n\n\n\n\nCharacteristic of testing vehicles. Soruce: Dallmann et al., 2018\n\n\n\nResult:\n\n\nDiesel passenger cars are six to seven times more likely to emit nitrogen oxides than petrol passenger cars.\n\n\n\n\n6 standard Euro NOx emission. Source: Dallmann et al., 2018\n\n\n\nEuro5 and Euro6 diesel engines emit significantly more nitrogen oxides than Euro3 and Euro4, while petrol engines emit the similar amount\n\n\n\n\naverage distance NOx emission for different vehicle family. Source: Dallmann et al., 2018\n\n\nTherefore, petrol vehicles outperform diesel vehicles and to achieve carbon emission reductions, the number of diesel vehicles needs to be limited and Euro3 or Euro4 diesel engines should be promoted instead of Euro5 and Euro6."
  },
  {
    "objectID": "week6.html#summary",
    "href": "week6.html#summary",
    "title": "7  week 6: Classification I",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis week is a classification task based on the application of Google Earth Engine.\nclassification algorithm: decision tree and random forest\n\n7.1.1 Decision tree\nDecision tree algorithm is a supervised learning algorithm that can be used for classification and regression problems. The algorithm works by recursively partitioning the feature space into smaller regions using a series of binary splits. At each split, the algorithm selects the feature that best separates the data into the target classes or predicts the target variable. The split is made based on the value of the selected feature, and the data is split into two or more subgroups based on the selected threshold value. This process is repeated until a stopping criterion is met, such as a minimum number of observations in a subgroup, or no further improvement in the classification or regression accuracy.\nThe decision tree algorithm creates a tree-like model of decisions and their possible consequences. Each internal node in the tree represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a predicted value. The algorithm can handle both categorical and numerical data, and can easily handle missing values and outliers. Decision trees are easy to interpret and visualize, and can be used for feature selection and variable importance analysis.\nOne of the most popular decision tree algorithms is the Classification and Regression Tree (CART) algorithm, which can handle both classification and regression tasks. Another popular algorithm is the ID3 algorithm, which is used for classification tasks only and is based on entropy and information gain measures. Other decision tree algorithms include C4.5, C5.0, and CHAID, which have different splitting criteria and pruning methods.\n\n\n7.1.2 Random forest\nRandom forest is a machine learning algorithm used for classification, regression, and other tasks. It is an ensemble learning method that combines multiple decision trees to make more accurate predictions.\nThe random forest algorithm works by constructing multiple decision trees during training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Each tree is constructed using a random subset of the training data and a random subset of the features. This helps to prevent overfitting and reduces variance in the predictions.\nDuring testing time, the random forest algorithm takes a new input and feeds it through each of the trees in the forest. Each tree then outputs a prediction, and the final prediction is based on the mode of the predictions from all of the individual trees.\nRandom forest has many advantages, including its ability to handle large datasets with high dimensionality and its ability to provide an estimate of the importance of each feature in the classification or regression task. It also has a low risk of overfitting compared to other algorithms.\nSome potential drawbacks of the random forest algorithm include its computational complexity and its tendency to perform poorly on datasets with imbalanced classes. However, with proper tuning and optimization, these limitations can be overcome"
  },
  {
    "objectID": "week6.html#application",
    "href": "week6.html#application",
    "title": "7  week 6: Classification I",
    "section": "7.2 Application",
    "text": "7.2 Application\n\n7.2.1 Vector data\nFirst of all, select the studying area based on the administrative unit layers data, my choice is Lhasa city, the capital city of Tibet. The longitude and latitude is [91.1457, 30.063].\nthis is the fundamental information:\n\nMap.setCenter(91.1457, 30.063, 8);\n\nvar styleParams = {\n  fillColor: 'b5ffb4',\n  color: '00909F',\n  width: 1.0,\n};\n\n// parameters fullfill the dataset\n// dataset = dataset.style(styleParams);\n\nMap.addLayer(dataset, {}, 'Second Level Administrative Units');\n\nvar lhasa = dataset.filter('ADM2_CODE == 13296');\n\n\n\n\nLhasa city information\n\n\n\n\n7.2.2 EO data\nThen generate the imagery with minimal cloud cover, at Lhasa, I use 1 as the parameter of cloudy pixel percentage, that means the cloud cover should be less than 1%. This is wayone, then, create another function maskS2clouds to mask the cloudy pixel and do the division, that is waytwo.\nwayone:\n\nvar divide10000 = function(image) {\n  return image.divide(10000);\n};\n\nvar wayone = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n                  .filterDate('2022-01-01', '2022-10-31')\n                  .filterBounds(lhasa)  // Intersecting ROI\n                  // Pre-filter to get less cloudy granules.\n                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',1));\n\nvar wayone_divide = wayone.map(divide10000);\n// var wayone_divide_mean = wayone_divide.median()\n\nMap.centerObject(lhasa, 10);\n\n\n\n\nResult of wayone\n\n\nwaytwo:\n\nfunction maskS2clouds(image) {\n  var qa = image.select('QA60');\n\n  // Bits 10 and 11 are clouds and cirrus, respectively.\n  var cloudBitMask = 1 << 10;\n  var cirrusBitMask = 1 << 11;\n\n  // Both flags should be set to zero, indicating clear conditions.\n  var mask = qa.bitwiseAnd(cloudBitMask).eq(0)\n      .and(qa.bitwiseAnd(cirrusBitMask).eq(0));\n\n  return image.updateMask(mask).divide(10000);\n}\n\nvar waytwo = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n                  .filterDate('2022-01-01', '2022-10-31')\n                  .filterBounds(lhasa)  // Intersecting ROI\n                  // Pre-filter to get less cloudy granules.\n                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',20))\n                  .map(maskS2clouds);\n\n// var waytwo_mean = waytwo.median();\n                  \nMap.addLayer(waytwo, visualization, 'waytwoRGB');\n\n\n\n\nResult of waytwo\n\n\nAs a result, waytwo performs worse than wayone because waytwo has higher cloud cover than wayone. but wayone is nstill not an ideal picture.\n\n\n7.2.3 How to fix:\n\nmethod 1: use median() function\n\n\n// for example\nvar wayone_divide = wayone.map(divide10000);\nvar wayone_divide_mean = wayone_divide.median();\n\nMap.addLayer(wayone_divide_median, visualization, 'wayoneRGB');\n\nmethod 2: use cloud and shadow coverage script\n\n\n// This example demonstrates the use of the\n// COPERNICUS/S2_CLOUD_PROBABILITY dataset, the\n// ee.Algorithms.Sentinel2.CDI() method for computing a\n// cloud displacement index and directionalDistanceTransform()\n// for computing cloud shadows.\n\n\n// Sentinel-2 Level 1C data.  Bands B7, B8, B8A and B10 from this\n// dataset are needed as input to CDI and the cloud mask function.\n\n// Cloud probability dataset.  The probability band is used in\n// the cloud mask function.\n\n// Sentinel-2 surface reflectance data for the composite.\n\n\n// The ROI is determined from the map.\nvar roi = ee.Geometry.Point([91.5385, 30.2192]);\nMap.centerObject(roi, 10);\n\n// Dates over which to create a median composite.\nvar start = ee.Date('2022-01-29');\nvar end = ee.Date('2022-01-31');\n\n// S2 L1C for Cloud Displacement Index (CDI) bands.\ns2 = s2.filterBounds(roi).filterDate(start, end)\n    .select(['B7', 'B8', 'B8A', 'B10']);\n// S2Cloudless for the cloud probability band.\ns2c = s2c.filterDate(start, end).filterBounds(roi);\n// S2 L2A for surface reflectance bands.\ns2Sr = s2Sr.filterDate(start, end).filterBounds(roi)\n    .select(['B2', 'B3', 'B4', 'B5']);\n\n// Join two collections on their 'system:index' property.\n// The propertyName parameter is the name of the property\n// that references the joined image.\nfunction indexJoin(collectionA, collectionB, propertyName) {\n  var joined = ee.ImageCollection(ee.Join.saveFirst(propertyName).apply({\n    primary: collectionA,\n    secondary: collectionB,\n    condition: ee.Filter.equals({\n      leftField: 'system:index',\n      rightField: 'system:index'})\n  }));\n  // Merge the bands of the joined image.\n  return joined.map(function(image) {\n    return image.addBands(ee.Image(image.get(propertyName)));\n  });\n}\n\n// Aggressively mask clouds and shadows.\nfunction maskImage(image) {\n  // Compute the cloud displacement index from the L1C bands.\n  var cdi = ee.Algorithms.Sentinel2.CDI(image);\n  var s2c = image.select('probability');\n  var cirrus = image.select('B10').multiply(0.0001);\n\n  // Assume low-to-mid atmospheric clouds to be pixels where probability\n  // is greater than 65%, and CDI is less than -0.5. For higher atmosphere\n  // cirrus clouds, assume the cirrus band is greater than 0.01.\n  // The final cloud mask is one or both of these conditions.\n  var isCloud = s2c.gt(65).and(cdi.lt(-0.5)).or(cirrus.gt(0.01));\n\n  // Reproject is required to perform spatial operations at 20m scale.\n  // 20m scale is for speed, and assumes clouds don't require 10m precision.\n  isCloud = isCloud.focal_min(3).focal_max(16);\n  isCloud = isCloud.reproject({crs: cdi.projection(), scale: 20});\n\n  // Project shadows from clouds we found in the last step. This assumes we're working in\n  // a UTM projection.\n  var shadowAzimuth = ee.Number(90)\n      .subtract(ee.Number(image.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n\n  // With the following reproject, the shadows are projected 5km.\n  isCloud = isCloud.directionalDistanceTransform(shadowAzimuth, 50);\n  isCloud = isCloud.reproject({crs: cdi.projection(), scale: 100});\n\n  isCloud = isCloud.select('distance').mask();\n  return image.select('B2', 'B3', 'B4').updateMask(isCloud.not());\n}\n\n// Join the cloud probability dataset to surface reflectance.\nvar withCloudProbability = indexJoin(s2Sr, s2c, 'cloud_probability');\n// Join the L1C data to get the bands needed for CDI.\nvar withS2L1C = indexJoin(withCloudProbability, s2, 'l1c');\n\n// Map the cloud masking function over the joined collection.\nvar masked = ee.ImageCollection(withS2L1C.map(maskImage));\n\n// Take the median, specifying a tileScale to avoid memory errors.\nvar median = masked.reduce(ee.Reducer.median(), 8);\n\n// Display the results.\nvar viz = {bands: ['B4_median', 'B3_median', 'B2_median'], min: 0, max: 0.3};\nMap.addLayer(median, viz, 'median');\n\n\n\n7.2.4 Classification\n\n7.2.4.1 Clip\n\n// clip by the gadm administrative shape: wayone\nvar wayone_clip = wayone_divide_mean.clip(lhasa) \nMap.addLayer(wayone_clip, visualization, 'wayoneRGB_clip');\n\n\n\n\nGeographical topography of Lhasa City Administrative Region\n\n\n\n\n7.2.4.2 Training\nTraining the data is an interesting process because the method used is a decision tree, a supervised machine learning algorithm, the accuracy of model depends on the accuracy of labels, so the classification of the various different land types had to be accurate in order for the resulting model to work better. Based on this, I looked for d typical cases of different land types as the training set.\n\nurban area: (with high density) one community of Lhasa city center, Lhasa financial center, universities\nwater: the Namucuo in northern Lhasa, Lhasa river\nwetland: Lalu Wetland National Nature Reserve (sample many times)\nbare earth: Zayaba Monastery, mountain area\nforest: Lhasa Nimu National Forest Park,\nice land: the top of those mountains\nurban area: (with low density) living area, villages around the city center\n\n\nvar polygons = ee.FeatureCollection([\n  ee.Feature(water, {'class': 1}),\n  ee.Feature(urban_high, {'class': 2}),\n  ee.Feature(wetland, {'class': 3}),\n  ee.Feature(bare_earth, {'class': 4}),\n  ee.Feature(forest, {'class': 5}),\n  ee.Feature(ice_land, {'class':6}),\n  ee.Feature(urban_low, {'class':7})\n]);\n\n\n// Use these bands for classification.\nvar bands = ['B2', 'B3', 'B4'];\n// The name of the property on the points storing the class label.\nvar classProperty = 'class';\n\n// Sample the composite to generate training data.  Note that the\n// class label is stored in the 'landcover' property.\nvar training = wayone_clip.select(bands).sampleRegions({\n  collection: polygons,\n  properties: [classProperty],\n  scale: 10\n});\n\n\nprint(training, \"training\")\n\n\n\n\nfeatures example\n\n\nWithin acceptable computational limits, the same land type can be sampled multiple times, which can significantly improve the accuracy of the prediction results\n\n// Train a CART classifier.\nvar classifier = ee.Classifier.smileCart().train({\n  features: training,\n  classProperty: classProperty,\n});\n// Print some info about the classifier (specific to CART).\nprint('CART, explained', classifier.explain());\n\n\n// Classify the image.\nvar classified = wayone_clip.classify(classifier);\n\n\n// add output\nMap.centerObject(lhasa);\nMap.addLayer(classified, {min: 1, max: 7, palette: [\n  'ab0000',  // urban area with high density\n  '466b9f',  // open water\n  'b8d9eb',  // Woody wetlands\n  'b5c58f',  // Mixed forest\n  'b3ac9f',  //Barren land (rock/sand/clay)\n  '466b9f',  //Perennial ice/snow\n  'd99282'  //urban area: lower density\n  ]}, \"classified\");\n\n\n\nExample 1\n\n\n\n\nLhasa city center classification\n\n\nAs the picture shows, this is the centre of Lhasa, the red coloured part is the Lhasa River flowing through the city, the blue is the main city with a high population density, while the white part in the upper left corner is a larger wetland, the periphery of the city centre is bare ground.\n\nExample 2\n\n\n\n\nNamucuo and mountain\n\n\nIn this example, the red part represents the water part and the blue part is the mountain peak and is above 6,000m, therefore the peaks are covered in snow all year round. Comparing this with the map, I found that the area is the Nyingchi Tanggula range, where you can see that the ridges run from north-east to south-west, and the green parts are mixed coniferous forests, located on the mountainsides and at the foot of the mountains. The thin blue line in the bottom right corner is the Qinghai-Tibet Highway, which runs along the Nyingchi Tanggula Range.\nHowever, this method depends on the accuracy of the training data I selected before, I introduced another more accurate method.\n\n\n7.2.4.3 training and testing data (use pixel method)\n\n////Pixel approach/////////////\n\nvar pixel_number= 1000;\n\nvar water_points=ee.FeatureCollection.randomPoints(water, pixel_number).map(function(i){\n  return i.set({'class': 1})})\n  \nvar urban_high_points=ee.FeatureCollection.randomPoints(urban_high, pixel_number).map(function(i){\n  return i.set({'class': 2})})\n  \nvar wetland_points=ee.FeatureCollection.randomPoints(wetland, pixel_number).map(function(i){\n  return i.set({'class': 3})})\n\nvar bare_earth_points=ee.FeatureCollection.randomPoints(bare_earth, pixel_number).map(function(i){\n  return i.set({'class': 4})})\n  \n// var forest_points=ee.FeatureCollection.randomPoints(forest, pixel_number).map(function(i){\n//   return i.set({'class': 5})})\n\n// var ice_land_points=ee.FeatureCollection.randomPoints(ice_land, pixel_number).map(function(i){\n//   return i.set({'class': 6})})\n\n// var urban_low_points=ee.FeatureCollection.randomPoints(urban_low, pixel_number).map(function(i){\n//   return i.set({'class': 7})})\n\nvar point_sample=ee.FeatureCollection([\n                                  urban_high_points,\n                                  water_points,\n                                  wetland_points,\n                                  bare_earth_points\n                                  // forest_points,\n                                  // ice_land_points\n                                  // urban_low_points\n                                  ])\n                                  .flatten()\n                                  .randomColumn();\n\n// assign 70% of training points to validation \nvar split=0.7\nvar training_sample = point_sample.filter(ee.Filter.lt('random', split));\nvar validation_sample = point_sample.filter(ee.Filter.gte('random', split));\n\n// take samples from image for training and validation  \nvar training = wayone_clip.select(bands).sampleRegions({\n  collection: training_sample,\n  properties: ['class'],\n  scale: 10,\n});\n\nvar validation = wayone_clip.select(bands).sampleRegions({\n  collection: validation_sample,\n  properties: ['class'],\n  scale: 10\n});\n\n// Random Forest Classification\n\nvar rf1_pixel = ee.Classifier.smileRandomForest(300)\n    .train(training, 'class');\n\n// Get information about the trained classifier.\nprint('Results of RF trained classifier', rf1_pixel.explain());\n\n\n// // --------------------- Step 3: Conduct classification --------------------------------\n    \nvar rf2_pixel = wayone_clip.classify(rf1_pixel);\n\nMap.addLayer(rf2_pixel, {min: 1, max: 5, palette: [\n  'ab0000',  // urban area with high density  urban high 2\n  '466b9f',  // open water  water  1\n  'b8d9eb',  // Woody wetlands  wetland 3\n  // 'b5c58f',  // Mixed forest   bare_earth 4 \n  'b3ac9f'  //Barren land (rock/sand/clay)  forest 5\n  // '466b9f'  //Perennial ice/snow    ice land 6\n  // 'd99282'  //urban area: lower density ruban low \n  ]}, \"RF_pixel\");  \n  \n\n// // --------------------- Step 4: Assess Accuracy --------------------------------\n\nvar trainAccuracy = rf1_pixel.confusionMatrix();\nprint('Resubstitution error matrix: ', trainAccuracy);\nprint('Training overall accuracy: ', trainAccuracy.accuracy());\n\nvar validated = validation.classify(rf1_pixel);\n\nvar testAccuracy = validated.errorMatrix('class', 'classification');\nvar consumers=testAccuracy.consumersAccuracy()\n\nprint('Validation error matrix: ', testAccuracy);\nprint('Validation overall accuracy: ', testAccuracy.accuracy())\nprint('Validation consumer accuracy: ', consumers);\n\n\nResult:\nInitially with 7 categories, I found the accuracy of the validation dataset to be around 80% and the OOB error to be around 17%, independent of both how many datasets were divided into training sets and how many trees were used in random forest. I realized it is possible to reduce the categories.\nWhen combining urban_high and urban_low, I saw a significant increase in those indicators, based on this finding, I then combine the forest and wetland land types, and compare the indicators with ice land and without ice land.\n\nout of bag error estimate: 7.56%\nTraining overall accuracy: 98.91%\nValidation overall accuracy: 92.38%\n\n\n\n\n4 categories\n\n\n\n\n\nLhasa city center under 4 categories\n\n\nThe results show that the indicators divided into four categories are better than the five categories. although 3 categories performs better than 4 categories, urban features and river features are not well identified and therefore 3 categories are not used.\n\nurban area\nwater area\nbare earth\nwetland\n\nIt is true that increasing the number of trees in a decision tree can help to improve accuracy, and increasing from 100 to 300 trees can improve metrics such as accuracy, but after increasing to 500 trees there is no significant change."
  },
  {
    "objectID": "week6.html#reflection",
    "href": "week6.html#reflection",
    "title": "7  week 6: Classification I",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nIn this week, I know how to select my own ROI, process graphics and classify different land type based on decision tree and random forest. As a supervised learning algorithm, the selection of training data input is one of the critical factors when training model, I always chose typical land types and got good results.\nBut both decision tree and random forest do not perform as well as dichotomous tasks for multi-category tasks."
  }
]