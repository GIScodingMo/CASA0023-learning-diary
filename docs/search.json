[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "1 Introduction\n\nThis book is a learning diary for CASA0023 Remotely Sensing Cities and Environments\n\n\n\n\nWhat is Remote Sensing"
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "2  Week1: Getting started with remote sensing",
    "section": "2.1 Summary",
    "text": "2.1 Summary\n\n2.1.1 The concept of remote sensing\nIn contrast to in situ or on-site observation, remote sensing is the process of gathering data about a phenomenon or object without actually coming into touch with it. The phrase is used particularly in reference to learning more about Earth and other planets.\nThe phrase “remote sensing” today typically refers to the detection and classification of Earthly objects using satellite- or aircraft-based sensor technologies. Based on transmitted signals, it encompasses the atmosphere, the surface, and the oceans (e.g. electromagnetic radiation). It can be divided into “active” and “passive” remote sensing (where a signal is emitted to the object by a satellite or aircraft and its reflection is detected by the sensor) (when the reflection of sunlight is detected by the sensor)\n\n\n\n2.1.2 Piece of work:\nIn the first chapter, I learned how to access remote sensing data using two open source databases, Landsat and Copernicus Open Access Hub, their existence make it easy for me to access remote sensing data from anywhere in the world.\nSecondly, I learned how to use QGIS, SNAP and some packages in R for simple raster processing calculations. I used my own case and followed the instructions step by step, learning a lot about the terminology, gaining some knowledge about remote sensing and statistics and also having some questions."
  },
  {
    "objectID": "week1.html#application",
    "href": "week1.html#application",
    "title": "2  Week1: Getting started with remote sensing",
    "section": "2.2 Application",
    "text": "2.2 Application\n\n2.2.1 QGIS\nWhen learning QGIS, I used as an example a map of Zhenjiang and Changzhou, two cities in Jiangsu Province, China, bordered by the Yangtze River to the north, from Copernicus Open Access Hub. Then, merged the B2 (blue), B3 (green), B4 (red) and B8 (NIR) in R10m with multiband colour, and knew the details in different enhancement options. Then, I learned about difference between R10m and R20m. Generally, upsampling is used to complement the situation when the pixel resolution is greater than the source resolution, and a common interpolation method is nearest neighbour, while downsampling is used when pixel resolution is smaller than source resolution. Here is the mechanism of nearest neighbour:\n\\(X_{src}=X_{dst}*\\frac{Width_{src}}{Width_{dst}}\\)\n\\(Y_{src}=Y_{dst}*\\frac{Height_{src}}{Height_{dst}}\\)\nIf the calculation results in a fractional number, then either remove the decimal part or round up to get the same value as the original dotted pixel. This method is very simple but inaccurate, the zoomed in image has a very bad mosaic and the zoomed out image has a very bad distortion. The problem lies in the treatment of the fractional part, which is equivalent to the process of colour gradation and cannot be fully equated with the existing colour of pixel. It is possible to use bilinear interpolation, considering the four values around the point that needs to be scaled, to see which value has more influence on it, and to give it more weight, I saw the option of bilinear interpolation then tried it. In addition, cubic has more accurate result because non-linear function can fit better but more complex.\n\n\n2.2.2 SNAP\nThe sRBG standard colour space can be used for display, network transmission and to convert colour levels on other devices into a colour space that is recognisable to the computer by means of the sRGB conversion function. This is why the brightness levels in the range [0,4095] on the Sentinel-2 can be converted to values in the range [0,255].\nWhen using SNAP, 432 nm means this wavelength is 432 nm, it is a coastal aerosol wave which wavelength is lessmthan blue wave. I like the colour composition function, which allows me to highlight different sections depending on my needs.\n\n\n\nB8-B4-B3\n\n\nHere is the scatter plot of my example:\n\n\n\nscatterplot\n\n\nIt can be seen that map of Zhenjiang and Changzhou has high brightness level, which means it is a city area, and it also shows a lot of wet bare soil, we can assume that these wet soils may be more suitable for cultivation, the overall biomass is not very large and, most notably, there is also a lot of dry bare soil lack of vegetation cover, presumably non-agricultural land.\nWhen using mask function, I got the administrative divisions of these municipalities because I downloaded the administrative divisions of China from GADM data:\n\n\n\nadministrative divisions\n\n\n(yellow: Zhenjiang, pink:Changzhou, red: Taizhou, green:Wuxi, blue: Yangzhou)\nAfter resampling and downscaling the data I got this tasselled plot, compared to the previous one I found that the downscaling had reduced the urban part, so the plot was significantly less bright, and there was more wet bare soil present than the previous one.\n\n\n\nscatterplot\n\n\nGenerally speaking, I prefer using SNAP than QGIS because it has more vivid chart and we can evaluate the local soil conditions, the size of the city and the biomass.\n\n\n2.2.3 R script\nWhen learning the code to deal with data from Landsat and Sentinel, I found a final paired t-test is performed to test whether the actual means of the urban land type from Landsat and Sentinel are equal, the degree of freedom from the results is 5970, p-value is much smaller than 0.05, which means the actual means of two groups are quite different, and at the same time, this calculation also shows the estimate mean of two samples. There are 95% probability that the true mean is between -9300.307 and -9198.368. data from Sentinel and Landsat is different."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "2  Week1: Getting started with remote sensing",
    "section": "2.3 Reflection",
    "text": "2.3 Reflection\nRemote sensing technology is now widely used in scientific research and practice, for land planning, geological exploration and forest fire prevention, so learning to make remote sensing maps is an essential skill.\nThere are several areas of interest to me, one of them is how PCA can be used to reduce the dimensionality of data, particularly in image processing, and I will be doing further study and figuring this out. Then, I would like to know if the image data from satellite remote sensing can be used in AI, as there are already mature deep learning algorithms with a high accuracy rate for image recognition, how to use these images to achieve functions such as automatic recognition and automatic monitoring."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "3  Week2: Portfolio",
    "section": "",
    "text": "Here is the link of presentation which is created via Xaringan: https://GIScodingMo.github.io/week2_xaringan"
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "4  Week3: Corrections",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nThis week I learned something about the corrections, the corrections can be divided into four aspects, geometric correction, atmospheric correction, orthorectification correction and radiometric calibration.\n\n4.1.1 Geometric correction\nWhy we need to do the geometric corrections:\n\nThe satellite has view angles when watching the Earth\nThe undulating nature of the terrain (topography)\nWind\nRotation of Earth\n\nThe methods to do the geometric correction:\n\nUse linear regression model to find the relationship between original points and distorted points\nBased on the linear relationship, it is better to use polynomial algorithms because it allows for a certain amount of curvature, it will be more accurate\nUse Thin Plate Spline algorithms to introduce the local deformations\n\n\n\n4.1.2 Atmospheric correction\nWe need to do the atmospheric correction to reduce the effect from path radiance and the haze created by absorption and scattering to get more accurate image\nThe methods to do the atmospheric correction:\n\nDark object subtraction (DOS), search the darkest value of image and subtract that from each pixel\nPsuedo-invariant features (PIFs) use the linear function to adjust the image\nACORN, FLAASH, QUAC, ATCOR\n\n\n\n4.1.3 Orthorectification correction\nThe aim of orthorectification correction:\n\nProvide the coordinates to an image\nRemove the distortions, make pixel viewed at straight down\n\nThe methods to do the orthorectification correction:\n\nConsider the solar zenith angle and solar azimuth angle to do the cosine correction\nThe software we can use QGIS, R packages like RStoolbox\n\n\n\n4.1.4 Radiometric calibration\nThe aim of radiometric calibration:\n\nConvert the digital number sensors captured from the image brightness (without units) to spectral radiance (with units)\n\nMethods to do the radiometric calbration:\n\nUse converting function"
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "4  Week3: Corrections",
    "section": "4.2 Application",
    "text": "4.2 Application\n\n4.2.1 DOS and radiance\nI used the remote sensing image (LC08_L1TP_119038_20230104_20230111_02_T1.txt downloaded from USGS EarthExplorer) to calculate the pixel value of surface reflections generated from scattering by DOS:\ninstall.packages(\"rgdal\")\ninstall.packages(\"RStoolbox\")\nlibrary(terra)\nlibrary(raster)\nlibrary(RStoolbox)\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(rgdal)\n\n## Import meta-data and bands based on MTL file\nmtlFile  <- (\"D:\\\\Remote Sensing\\\\week3\\\\LC08_L1TP_119038_20230104_20230111_02_T1\\\\LC08_L1TP_119038_20230104_20230111_02_T1_MTL.txt\")\n\nmetaData <- readMeta(mtlFile)\nlsatMeta  <- stackMeta(metaData)\n\n# surface reflection with DOS\nl8_boa_ref <- radCor(lsatMeta, metaData, method = \"dos\")\n\n# write to local dos\nterra::writeRaster(l8_boa_ref, datatype=\"FLT4S\", filename = \"D:\\\\Remote Sensing\\\\week3\\\\LC08_L1TP_119038_20230104_20230111_02_T1\\\\l8_boa_ref.tif\", format = \"GTiff\", overwrite=TRUE)\n\n# Radiance \nlsat_rad <- radCor(lsatMeta, metaData = metaData, method = \"rad\")\n\nterra::writeRaster(lsat_rad, datatype=\"FLT4S\", filename = \"D:\\\\Remote Sensing\\\\week3\\\\LC08_L1TP_119038_20230104_20230111_02_T1\\\\lsat_rad.tif\", format = \"GTiff\", overwrite=TRUE)\n\n\n\nl8_boa_ref\n\n\nConclusion:\nThere are several methods to execute the atmosphereic corrections (from digital number to reflectance), two typical examples are DOS and converting radiance to reflectance. In this case, the difference between radiance to reflectance and DOS correction is not significant and both can be corrected accurately. Actually, When the atmosphere is thinner or when the weather is good, the influence of the atmosphere on the ground radiation is less, the TOA and BOA are close to each other, a small part of the light reaches the sensor through the path radiance, so the less dark object needs to be substracted, the effect is close to the radiance. On the contrary, the more pixels need to be corrected the more obvious the effect of the corrected image is compared to the pre-correction image.\n# hazeDN\n\nhazeDN <- RStoolbox::estimateHaze(lsatMeta, hazeBands = 2:4, darkProp = 0.01, plot = TRUE)\n\nlsat_sref <- radCor(lsatMeta, metaData = metaData, method = \"dos\", \n                    hazeValues = hazeDN, hazeBands = 2:4)\n\nterra::writeRaster(lsat_sref, datatype=\"FLT4S\", filename = \"D:\\\\Remote Sensing\\\\week3\\\\LC08_L1TP_119038_20230104_20230111_02_T1\\\\lsat_sref.tif\", format = \"GTiff\", overwrite=TRUE)\n\n\n\nresult of hazeDN (3 Bands)\n\n\n\n\n4.2.2 Merging imagery\nLandsat8, Landsat9 and data preparation\n# List your raster files excluding band 8 using the patter argument\nlistlandsat_8<-dir_info(\"D:\\\\Remote Sensing\\\\week3\\\\LC08_L2SP_119038_20230104_20230111_02_T1\")%>%\n  dplyr::filter(str_detect(path, \"[B123456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  terra::rast()\n\n# List your raster files excluding band 8 using the patter argument\nlistlandsat_9<-dir_info(\n\"D:\\\\Remote Sensing\\\\week3\\\\LC08_L2SP_119038_20230104_20230111_02_T1\"\n)%>%\n  dplyr::filter(str_detect(path, \"[1B23456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  terra::rast()\n\n# data preparation\nm1 <- terra::mosaic(listlandsat_8, listlandsat_9, fun=\"mean\")\n\n\n4.2.3 Enhancement\nCalculate the NDVI (Normalized Difference Vegetation Index)\nm1_NDVI <- (m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5 - m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4) / (m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5 + m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4)\n\nm1_NDVI %>%\n  plot(.)\n\n\n\nresult of m1_NDVI\n\n\nSet m1_NDVI is not greater than 0.2\nveg <- m1_NDVI %>%\n  terra::classify(., cbind(-Inf, 0.2, NA))\n\nveg %>%\n  plot(.)\nThe highlight area of healthy vegetation:  From this map, the vegetation in the Taihu Lake basin is more concentrated to the west of the lake, where the most vegetation is found and where there is less population, while the area north of the lake to the south of the Yangtze River is less vegetated due to the high level of urbanisation, while the area north of the Yangtze River is relatively more vegetated.\nCalculate the NDMI (Normalized Difference Moisture Index) Use band 5 and band 6 on the occasion of Landsat8:\n\\(NDMI = (Band5 - Band6) / (Band5 + Band6)\\)\nm1_NDMI <- (m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5 - m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B6) / (m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5 + m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B6)\n\nm1_NDMI %>%\n  plot(.)\n\n\n\n\nresult of m1_NDMI\n\n\nmoi <- m1_NDMI %>%\n  terra::classify(., cbind(-Inf, 0.05, NA))\n\nmoi %>%\n  plot(.)\n ### Filtering\n# for a 3 by 3 filter on Band4\nm1_filter <- terra::focal(m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4, w=matrix(nrow=3,ncol=3))\n\n\n4.2.4 Texture analysis\nGenerate the glcm.red and glcm.nir from Band4 and Band5, see the result under different statistical indicators\ninstall.packages(\"glcm\")\n\n\nlibrary(glcm)\nlibrary(raster)\n\n# band4 red, band5 NIR\n\nband4_raster <- raster::raster(m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4)\nband5_raster <- raster::raster(m1$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5)\n\nglcm.red <- glcm(band4_raster,\n                   window = c(7, 7),\n                   #shift=list(c(0,1), c(1,1), c(1,0), c(1,-1)), \n                   statistics = c(\"mean\",\n                                  \"variance\",\n                                  \"homogeneity\",\n                                  \"contrast\",\n                                  \"entropy\", \n                                  \"dissimilarity\",\n                                  \"second_moment\", \n                                  \"correlation\"))\n\n\nglcm.nir <- glcm(band5_raster,\n                   window = c(7, 7),\n                   #shift=list(c(0,1), c(1,1), c(1,0), c(1,-1)), \n                   statistics = c(\"mean\",\n                                  \"variance\",\n                                  \"homogeneity\",\n                                  \"contrast\",\n                                  \"entropy\", \n                                  \"dissimilarity\",\n                                  \"second_moment\", \n                                  \"correlation\"))\n\n\n\nplot(glcm.red)\n\n\n\nresult of glcm.red\n\n\nplot(glcm.nir)\n\n\n\nresult of glcm_nir\n\n\n\n\n4.2.5 Data fusion and PCA\n# for the next step of PCA we need to keep this in a raster format\n\n# m1_raster, glcm.red\nm1_raster <- stack(m1)\nFuse <- stack(m1_raster, glcm.red)\n\nFuse_3_bands <- stack(Fuse$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B4, Fuse$LC08_L2SP_119038_20230104_20230111_02_T1_SR_B5, Fuse$glcm_homogeneity)\n\nscale_fuse <- scale(Fuse_3_bands)\n\n# m1_raster, glcm.red\npca <- rasterPCA(Fuse, nSamples =100, spca = TRUE)\n\nsummary(pca$model)\n\n\n\nresult of pca (m1_raster, glcm.red)\n\n\nIt can be seen that in my study area, the component 1 accounts for 54.20%, and the summary of component 1 to component 3 accounts for over 90%, component 1 can explain the 54.2% data from entire dataset\nplot(pca$map)\n\n\n\nplot of PCA each component (m1_raster, glcm.red)\n\n\n# glcm.red, glcm.nir\nFuse <- scale(stack(glcm.red, glcm.nir))\n\npca <- rasterPCA(Fuse, nSamples = 100, spca = TRUE)\n                 \nsummary(pca$model)\n\n\n\nresult of pca (glcm.red, glcm.nir)\n\n\nThe component 1 of glcm.red and glcm.nir is 36.63%, the result is not so good\nplot(pca$map)"
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "4  Week3: Corrections",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nIn this week, I learned the knowledge about the corrections, and the methods about corrections, the distortion of the images from remote sensing is mainly due to the presence of sensor observations in terms of declination, topography, wind and so on. Their correction uses statistical knowledge and principles (such as PCA, linear regression) as well as knowledge of spatial geometry.\nI think it is useful for the further study and work, for example, correcting the satellite image to satisfy people’s demand."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "5  week 5: Policy",
    "section": "",
    "text": "6 city plan and application of remote sensing for London"
  },
  {
    "objectID": "week4.html#summary",
    "href": "week4.html#summary",
    "title": "5  week4: Policy",
    "section": "5.1 Summary",
    "text": "5.1 Summary\n\n5.1.1 Current city layout of London\nGeneral speaking, London has a complex city layout that has evolved over centuries of growth and development. Until 2022, London was a sprawling metropolis, with a mix of historic and modern buildings, a well-developed transport network, and a diverse range of neighborhoods and communities.\n\nmain functional areas The city is divided into 32 boroughs, each with its own unique character and amenities. For example, the center of London is home to many of the city’s famous landmarks and tourist attractions, including Buckingham Palace, the Tower of London, and the London Eye. The West End is known for its theaters, shopping, and nightlife, while the South Bank is home to many cultural institutions, including the National Theatre and the Tate Modern art museum.\n\n\n\n\nLondon borough map. Source: London Communications Agency\n\n\n\ndevelopment direction: London is surrounded by green belt land (dark green section), which serves to limit the expansion of the city and protect the surrounding countryside. Despite this, the city continues to grow, with new developments and communities being built on the outskirts of London, particularly in the east and southeast of the city. The light green section represents the areas of London that are still to be developed.\n\n\n\n\nGreen belt land surrounded London. Source: The London Plan 2021\n\n\n\ntransportation: The transport network in London is extensive and includes an underground railway system (the London Underground), a network of bus routes, and overground rail services. London is also well connected to the rest of the UK and Europe, with several major airports, including Heathrow and Gatwick, and international train services from St Pancras station.\n\nIt includes several kinds of means of transport as following: London Underground (Tube): The London Underground is a rapid transit system that operates throughout the city and is one of the oldest and largest metro systems in the world. It comprises of 11 lines and serves 270 stations.\n\nBuses: London has one of the largest urban bus networks in the world, with over 8,000 buses serving over 700 routes. Buses are an important mode of transportation for many Londoners and are a cost-effective way to travel around the city.\nOverground: London Overground is a network of suburban rail services that operates across Greater London and the surrounding areas. It connects with the London Underground and National Rail services, providing a comprehensive rail network for Londoners.\nTaxis and Private Hire Vehicles: London is famous for its black cabs, which can be hailed on the street or at taxi ranks. Private hire vehicles, also known as minicabs, must be pre-booked and are widely available throughout the city.\nCycling: London has a growing number of cycle lanes and cycle-friendly streets, and there are several schemes in place to encourage cycling, including the Santander Cycles bike hire scheme.\nWalking: London is a walkable city, and many areas are easily accessible on foot. There are also several pedestrian-only zones, making it easier and safer for people to walk around the city.\n\n\n\n5.1.2 development goals and policies in the future\n\nProvide housing for a growing population: The plan aims to deliver new homes to meet the growing demand for housing in the city.\nPromote economic growth: The plan aims to support London’s economy by creating new jobs, promoting investment, and supporting businesses. improve the efficiency and resilience.\nImprove transport and connectivity: The plan aims to make it easier for people to travel around London and connect with the rest of the country and beyond.\nEnhance the environment: The plan aims to improve the quality of the natural and built environment in London and to protect the city’s green spaces.\nPromote social inclusion and equality: The plan aims to ensure that everyone in London has the opportunity to participate in the city’s social, economic, and cultural life.\n\n\n\n5.1.3 Problems during city development of London\n\nHousing Shortage: London has faced a persistent shortage of affordable housing, particularly in the central areas of the city. This has led to high levels of homelessness and housing insecurity, and has put pressure on the city’s social and healthcare systems. In addition, with the CPI and cost of living growth, it becomes more and more difficult for foreigners to live there, high rental, high energy cost, high transportation cost make them leave.\nCongestion and Traffic: London’s rapid growth has led to increased traffic congestion, air pollution, and noise pollution. The city has struggled to keep up with the demands of its growing population and has faced challenges in improving the efficiency and sustainability of its transportation network.\n\n\n\n\nAir quality focus area. Source: The London Plan 2021\n\n\n\nInadequate Infrastructure: London’s infrastructure has struggled to keep pace with the city’s growth, and there have been significant challenges in providing adequate public transportation, water and sewage services, and waste management facilities.\nEnvironmental Degradation: London’s rapid growth has led to the degradation of its natural environment, including the loss of green spaces, air and water pollution, and the degradation of its soil and vegetation.\nSocial Disparities: London is a city with significant social and economic disparities, with some areas experiencing high levels of poverty, crime, and social exclusion, while others are more affluent and economically prosperous.\nHistorical Preservation: London is a city with a rich history and cultural heritage, and there have been challenges in balancing the need to preserve its historical buildings and monuments with the demands of a modern, growing city."
  },
  {
    "objectID": "week4.html#application",
    "href": "week4.html#application",
    "title": "5  week4: Policy",
    "section": "5.2 Application",
    "text": "5.2 Application\n\n5.2.1 Case 1: Summer heat spots from Landsat8 Thermal satellite data\nLandsat8 satellite can provide a high-resolution thermal data and use as a basis for mapping the spatial distribution of Great London Area (GLA) surface temperatures and identifying those urban hotspots. This dataset is a count of surface temperatures from the summer months (June, July and August only) between 2016 and 2020, obtained from landsat8 thermal infrared imagery. In addition to the five-year average temperature for the area, the dataset also includes maximum and minimum temperatures and standard deviations to provide a visual representation of the magnitude and range of temperature changes over the five-year period in GLA.\nHigher temperatures tend to occur in densely populated areas.\n\n\n\nheat spot of London summer temperature. Data source: Major Summer Heat Spot London Datastore\n\n\nAs can be seen from the map, the population is predominantly distributed between the north and south banks of the Thames, with the population on the north bank being greater than that on the south bank. The green belt around London can be seen more clearly on the map. The population of the north bank is more dense in the east than in the west. The density of population is high from the City of London all the way north, and is also high in north-east London, decreasing further east. Conversely, the centre of the South Bank is more densely populated, while the east and west are less dense. If Greater London continues to expand, the north-east and south-east would be a better choice.\nTake the City of London for example, some areas saw the highest temperatures compared with the whole city, such as Euston Station east of Regent’s Park, and most of the high heat areas were in high-traffic areas such as train stations, tube stations, shopping malls and airports.\n\n\n\nheat spot in City of London\n\n\nIn addition to population density affecting surface temperatures, industry, airport also have an impact on surface temperatures, with the highest surface temperatures occurring in the five year period shown below at the Dagenham engine plant, ocado customer fullfilment centre, ExCel International Convention and Exhibition Centre along the Thames in east London, and Heathrow Airport to the west.\n\n\n\nHighest temperature areas (east)\n\n\n\n\n\nHighest temperature areas (west)\n\n\nKnowing the temperature changes will help to understand the population distribution in GLA, which in turn will allow transport to be organised to develop faster and more accessible transport in dense areas, to develop infrastructure and to reduce the pressure on existing transport (for example, the central line passes through areas of great population density, and the central line is the oldest tube in London and still the busiest in London). The central line, for example, is the oldest and still the busiest tube in London, so it has to take on a lot of traffic pressure). In addition, more polluting facilities such as factories can be relocated to less populated areas. Knowing the distribution of temperatures can also control the creation of high temperature situations, such as the rare high temperatures in the London area in the summer of 2022.\n\n\n5.2.2 case 2: Remote sensing of motor vehicle emissions in London\nenergy hierarchy policy and target of London city plan * be lean: less energy used and energy demand management * be clean: fully use local energy (secondary energy explicit) and use energy more efficiently and cleanly. * be green: add the opportunity for using renewable energy, provide producing, storing energy on-site * be seen: monitor, verify and report on energy performance.\n\n\n\nEnergy hierarchy\n\n\nIn response to a call for collaboration between the Mayor of London and the TRUE initiative, carbon emissions from passing vehicles were tested using remote sensing technology at nine sites across Greater London between 2017 and 2018, with carbon emissions data recorded for over 100,000 vehicles, The experiment focused on petrol and diesel vehicles in the London area, with vehicle types including passenger cars, buses, light passenger vehicles, trucks and motorbikes, and measured emissions of carbon oxides and nitrogen oxides.\nThe remote sensing equipment used for the experiments was the Opus AccuScan RSD5000, which was the first to test exhaust gases in three main ways:\n\nThe device emits infrared and ultraviolet light velocities that pass through vehicle emissions, measuring the attenuation of these beams, instant vehicle emissions, the device measures nitrogen oxides and carbon oxides, opacity is measured as a proxy for respirable particulate matter, the device emits a frequency of 200 Hz and can measure 100 times in 0.5 seconds.\nMeasurement of the instantaneous acceleration of the vehicle as a measure of the engine load, which is related to the instantaneous emission rate.\nOne camera is responsible for photographing the vehicle licence plate for database comparison to determine its model, displacement standard.\n\nThe general conditions of testing vehicles\n\n\n\nCharacteristic of testing vehicles\n\n\nResult:\n\nDiesel passenger cars are six to seven times more likely to emit nitrogen oxides than petrol passenger cars.\n\n\n\n\n6 standard Euro NOx emission. Source: Dallmann et al., 2018\n\n\n\nEuro5 and Euro6 diesel engines emit significantly more nitrogen oxides than Euro3 and Euro4, while petrol engines emit the similar amount\n\n\n\n\naverage distance NOx emission for different vehicle family. Source: Dallmann et al., 2018\n\n\nTherefore, petrol vehicles outperform diesel vehicles and to achieve carbon emission reductions, the number of diesel vehicles needs to be limited and Euro3 or Euro4 diesel engines should be promoted instead of Euro5 and Euro6."
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "5  week4: Policy",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nIn this week, I learned a lot about London city plan in future decades, first the current situation of London development, then the problems of city development, the target city plan, and use several cases to demonstrate how remote sensing technology can help with city plan.\nFirst, use the temperature heat spot case to find the pattern of London population distribution, this helps to enhance the efficiency and effectiveness of London land usage. Then, use a kind of remote sensing equipment to measure the carbon emission of different vehicles, and come up with some constructive advice to acheive lower carbon target."
  },
  {
    "objectID": "week4.html#application-case-study",
    "href": "week4.html#application-case-study",
    "title": "5  week4: Policy",
    "section": "5.2 Application (case study)",
    "text": "5.2 Application (case study)\nCase 1:"
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "6  week5: Google Earth Engine",
    "section": "6.1 Summary",
    "text": "6.1 Summary\n\n6.1.1 Introduction about Google Earth Engine\nGoogle Earth Engine is a cloud-based platform for geospatial data analysis and visualization. It provides access to a vast repository of satellite imagery and geospatial data, as well as a powerful suite of tools for processing and analyzing this data.\nThe advantages of Earth Engine is scalability. The platform can handle extremely large datasets and perform complex analysis in near real-time, making it well suited for monitoring changes on a global scale. Earth Engine also offers an easy-to-use programming interface (use command-based programming, env: javascript), allowing users to write custom scripts and algorithms to perform specific analysis tasks.\nGoogle Earth Engine is also widely used by government agencies, NGOs, and academic institutions for a variety of purposes, including environmental monitoring, natural resource management, and disaster response. The platform is free to use for non-commercial purposes, and access to its data and tools can be granted through a simple application process.\n\n\n6.1.2 Application of Google Earth Engine\n\nEurope’s Air Quality\n\nFrom Google Earth Engine, we can see the air quality monitor of over than 30 countries in Europe, it has several standards, the maximum cloud allowance, reducer scale, we can compare the absolute difference and relative difference, and find the top and the last countries.\n\nNDVI slider\n\nThis is the map of vegetation distribution all over the world, From this map we can see where on which continents the vegetation is dense and where it is likely to be desert and saline, and it also supports the comparison between different years to see the trend of vegetation distribution on the surface of earth."
  },
  {
    "objectID": "week5.html#application",
    "href": "week5.html#application",
    "title": "6  week5: Google Earth Engine",
    "section": "6.2 Application",
    "text": "6.2 Application\n\n6.2.1 Single point selection\nI used Changzhou city, Jiangsu Province, China as my ROI, and select the point [119.9698545084502,31.780198098728107]\n\n\n\nROI\n\n\nThen use Landsat9 surface reflectance dataset (LANDSAT/LC09/C02/T1_L2), filter date is from 2020-01-01 to 2023-02-10, but when cloud cover is 0.1, I can not get any image, so I tried to adjust the cloud cover parameter to 0.4, I got two images, the cloud cover is 0.24 and 0.32, that is not cloud free Landsat9 scene. Until now, I have got two problems:\n\nCloud cover is larger than 0.1\nOnly show one single point, not the whole city\n\n\n\n\ncloud cover\n\n\nTo fix these problem, I use GADM data and choose gadm4_CHN_2 to display the city boundary and load the image:\nvar Changzhou = ee.FeatureCollection('users/giscodingmo/gadm41_CHN_2')\n    .filter('NL_NAME_1 == \"常州市\"');\n\nvar oneimage_study_area_cloud = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n.filterDate('2021-01-01', '2022-12-10')\n.filterBounds(Changzhou)  // Intersecting ROI\n.filter(ee.Filter.lt(\"CLOUD_COVER\", 0.1));\n\n\n// load the image\nvar image_120_38 = ee.Image('LANDSAT/LC08/C02/T1_L2/LC08_120038_20211004')\n\n// add the image on the map\nMap.addLayer(one_image, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]}, \"Landsat 8\")\nRresult: only one image in Landsat8 can reach the standard, and the image id is LANDSAT/LC08/C02/T1_L2/LC08_120038_20211004, which means path 120, row 38\n\n\n\npath and row\n\n\n\n\n6.2.2 Discussion of median method\nInitially, I use imageCollection.reduce() function (median method) to reduce the images:\nvar median = oneimage_study_area_cloud.reduce(ee.Reducer.median());\n\n// print the image info\nprint(median, \"median\")\n\nMedian method is effective when the classification task is between those objects which have giant jump such as forest and non-forest area, water and non-water area.\nIf the gap is not obvious, it is difficult to use median method to integrate images. To improve the accuracy of the classification, an improved method is to add other quartiles to the median, e.g. 25% quartile, 75% quartile. Another method is using seasonal median (spring, summer, fall, winter) to substitute single median, use a curve to fit the trend.\n// example of season medians\nfunction seasonComposite(start) {\n  var end = ee.Number(start).add(2)  \n  // transfer the data type\n  return collection\n  .filter(ee.Filter.calendarRange(start, end, \"month\"))\n  .median()\n}\n\n// call\nvar seasons = ee.List([1,4,7,10]).map(seasonComposite);\nvar composite = ee.ImageCollection(seasons).toBands();\n\n\n6.2.3 Better images\n\n6.2.3.1 True color image\nwrite a function for the surface reflectance rate and temperature adjustment in Landsat Collection 2\n// Applies scaling factors in Collection 2\nfunction applyScaleFactors(image) {\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  var thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n  return image.addBands(opticalBands, null, true)\n              .addBands(thermalBands, null, true);\n}\n\n// call our collection to the function and assign it to a new variable \nvar oneimage_study_area_cloud_scale = oneimage_study_area_cloud.map(applyScaleFactors);\n\n// apply the median reducer\nvar oneimage_study_area_cloud_scale_median = oneimage_study_area_cloud_scale.reduce(ee.Reducer.median());\n\nprint(oneimage_study_area_cloud_scale_median)\n\n\n// set up some of the visualisation paramters \nvar vis_params = {\n  bands: ['SR_B4_median', 'SR_B3_median', 'SR_B2_median'],\n  min: 0.0,\n  max: 0.3,\n};\n\n// add a layer to the map\nMap.addLayer(oneimage_study_area_cloud_scale_median, vis_params, 'True Color (432)');\nResult:\n\n\n\nvisualization by true color (432)\n\n\n\n\n6.2.3.2 Mosaic images\nvar mosaic = oneimage_study_area_cloud_scale.mosaic();\n\nvar vis_params2 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.addLayer(mosaic, vis_params2, 'spatial mosaic');\n\nResult:\n\n\n\nmosaic picture\n\n\nthere is no significant difference when using single image mosaic model and true color (432) model.\n\n\n6.2.3.3 Clip image\nvar clip = meanImage.clip(Changzhou)\n  .select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']);\n\nvar vis_params3 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0,\n  max: 0.3,\n};\n\n// map the layer\nMap.addLayer(clip, vis_params3, 'clip');\nResult: the image boundary is followed by city boundary\n\n\n\ncity boundary clip image\n\n\n\n\n\n6.2.4 Texture measures\n// based on the clip generated before\nvar glcm = clip.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'])\n  .multiply(1000)\n  .toUint16()\n  .glcmTexture({size: 1})\n  .select('SR_.._contrast|SR_.._diss')\n  .addBands(clip);\n  \n// add to the map, but change the range values  \nMap.addLayer(glcm, {min:15, max:650 }, 'glcm');\n\nResult:\n\n\n\ntexture adjustment\n\n\n\n\n\npixel distribution under glcm image\n\n\n\n\n6.2.5 PCA\nPrinciple component analysis code is here:\nvar scale = 30;\nvar bandNames = glcm.bandNames();\n\n// print(bandNames)\n\nvar region = Changzhou.geometry();\n// Map.centerObject(region, 10);\n// Map.addLayer(ee.Image().paint(region, 0, 2), {}, 'Region');\n\n// print(region, \"Changzhou_geometry\")\n// print(bandNames)\n\n// mean center the data and SD strech the princapal components \n// and an SD stretch of the principal components.\nvar meanDict = glcm.reduceRegion({\n    reducer: ee.Reducer.mean(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n});\n\nvar means = ee.Image.constant(meanDict.values(bandNames));\nvar centered = glcm.subtract(means);\n\n\n// This helper function returns a list of new band names.\nvar getNewBandNames = function(prefix) {\n  var seq = ee.List.sequence(1, bandNames.length());\n  return seq.map(function(b) {\n    return ee.String(prefix).cat(ee.Number(b).int());\n  });\n};\n\n\n// This function accepts mean centered imagery, a scale and\n// a region in which to perform the analysis.  It returns the\n// Principal Components (PC) in the region as a new image.\nvar getPrincipalComponents = function(centered, scale, region) {\n  // Collapse the bands of the image into a 1D array per pixel.\n  var arrays = centered.toArray();\n\n  // Compute the covariance of the bands within the region.\n  var covar = arrays.reduceRegion({\n    reducer: ee.Reducer.centeredCovariance(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n  });\n\n  // Get the 'array' covariance result and cast to an array.\n  // This represents the band-to-band covariance within the region.\n  var covarArray = ee.Array(covar.get('array'));\n\n  // Perform an eigen analysis and slice apart the values and vectors.\n  var eigens = covarArray.eigen();\n\n  // This is a P-length vector of Eigenvalues.\n  var eigenValues = eigens.slice(1, 0, 1);\n  // This is a PxP matrix with eigenvectors in rows.\n  \n  var eigenValuesList = eigenValues.toList().flatten()\n  var total = eigenValuesList.reduce(ee.Reducer.sum())\n  var percentageVariance = eigenValuesList.map(function(item) {\n  return (ee.Number(item).divide(total)).multiply(100).format('%.2f')\n    })\n  \n  print(\"percentageVariance\", percentageVariance)  \n\n  var eigenVectors = eigens.slice(1, 1);\n\n  // Convert the array image to 2D arrays for matrix computations.\n  var arrayImage = arrays.toArray(1);\n\n  // Left multiply the image array by the matrix of eigenvectors.\n  var principalComponents = ee.Image(eigenVectors).matrixMultiply(arrayImage);\n\n  // Turn the square roots of the Eigenvalues into a P-band image.\n  var sdImage = ee.Image(eigenValues.sqrt())\n    .arrayProject([0]).arrayFlatten([getNewBandNames('sd')]);\n\n  // Turn the PCs into a P-band image, normalized by SD.\n  return principalComponents\n    // Throw out an an unneeded dimension, [[]] -> [].\n    .arrayProject([0])\n    // Make the one band array image a multi-band image, [] -> image.\n    .arrayFlatten([getNewBandNames('pc')])\n    // Normalize the PCs by their SDs.\n    .divide(sdImage);\n};\n\n// Get the PCs at the specified scale and in the specified region\nvar pcImage = getPrincipalComponents(centered, scale, region);\n\n// Plot each PC as a new layer\nfor (var i = 0; i < bandNames.length().getInfo(); i++) {\n  var band = pcImage.bandNames().get(i).getInfo();\n  Map.addLayer(pcImage.select([band]), {min: -2, max: 2}, band);\n}\n\nResult: I got the pictures, the number is equal to the length of bandNames, in my case, the bandNames contains 21 elements.\n\n\n\nexample of PCA analysis\n\n\nThe first component can explain 63.68% of variance within the collection and the second component explains 26.66%, the third explains only 6.11%, therefore, I can just add pc1 and pc2 instead of the entire image, somethimes, pc3 can also be added to improve the accuracy, reaching 96.45%.\n\n\n\nPercentage variables of PCA\n\n\nMap.addLayer(pcImage, {bands: ['pca3', 'pc2', 'pc1'], min: -2, max: 2}, 'PCA bands 1, 2 and 3');\n\nMap.addLayer(pcImage, {bands: ['pc2', 'pc1'], min: -2, max: 2}, 'PCA bands 1, 2');\nComparison between 2 components and 3 components:\n\n\n\n2 components\n\n\n\n\n\n3 components\n\n\nExport image to drive\nvar PCA_out = pcImage.select(['pc1', 'pc2', 'pc3'])\n\nvar projection = PCA_out.select('pc1').projection().getInfo();\n\nvar bounds = Changzhou.geometry();\n\n// Export the image, specifying the CRS, transform, and region.\nExport.image.toDrive({\n  image: PCA_out,\n  description: 'PCA_Changzhou',\n  scale:30,\n  crs: projection.crs,\n  maxPixels: 100E10,\n  region: bounds\n});\n\n\n6.2.6 Band math\nCalculate the NDVI quickly, use blue to identify the water area, green to identify the forest or vegetation area and white to identify the town, city\n//NDVI\nvar NDVI_1 = clip.select('SR_B5').subtract(clip.select('SR_B4'))\n  .divide(clip.select('SR_5').add(clip.select('SR_B4')));\n\nMap.addLayer(NDVI_1, { min: -1, max: 1, palette: ['blue', 'white', 'green']}, 'NDVI');\n\n\n\nNDVI"
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "6  week5: Google Earth Engine",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nIn this week, I learned knowledge in several aspects: * how to use Google Earth Engine to create points and analyse the ROI assisted with GADM boundary map.\n\nunderstood the advantages and disadvantages of using median methods to get the reduced images (advantages: fast calculation, be suitable for those objects which has giant difference; disadvantages: not accurate, can not reflect the trend of change) and knew the alternative and more accurate methods (multiple quantile methods: using 10% quantile, 25% quantile, 75% quantile, seasonal medians methods: using four medians to represent 4 seasons)\ncreate true color image, mosaic image, clip image and do the texture adjustment by glcm\nprinciple component analysis and find the explainable percentage, calculated NDVI and export them to local"
  }
]